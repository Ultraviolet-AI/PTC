{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install --upgrade pip # else errors out\n",
    "# home/tools/data/propaganda-techniques-names-semeval2020task11.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "# Load existing model -- still doesn't work\n",
    "# args['output_dir'] = '/floyd/home/datasets/output/TC_model'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args['output_dir'])\n",
    "# model = AutoModel.from_pretrained(args['output_dir'])\n",
    "# model.to(device)\n",
    "# torch.load(os.path.join(args['output_dir'], 'training_args.bin')) \n",
    "\n",
    "\n",
    "## Bo's method -- better\n",
    "# state = torch.load('/floyd/home/datasets/output/TC_model/tc.pth', map_location=device)\n",
    "\n",
    "# model.load_state_dict(state, strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SI Instantiate the model\n",
    "# config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
    "# config = config_class.from_pretrained(args[\"model_name\"], num_labels=2, finetuning_task=\"binary\")\n",
    "# tokenizer = tokenizer_class.from_pretrained(args[\"model_name\"])\n",
    "# model = model_class(config)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TC Instantiate the model\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
    "config = config_class.from_pretrained(args[\"model_name\"], num_labels=len(PROP_TECH_TO_LABEL))\n",
    "tokenizer = tokenizer_class.from_pretrained(args[\"model_name\"])\n",
    "model = model_class(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args['output_dir'] = '/floyd/home/datasets/output/TC_model'\n",
    "\n",
    "# Make sure to change the output directory to not overwrite SI model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOG:Generating training dataset...\n",
      "INFO:LOG:Generating dataframe for folder /floyd/home/datasets/train-articles\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bcb37bc19828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_training_dataset_from_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_articles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_TC_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving model checkpoint to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model_to_save = model.module if hasattr(model, 'module') else model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/floyd/home/notebooks/preprocess.py\u001b[0m in \u001b[0;36mgenerate_training_dataset_from_articles\u001b[0;34m(articles_folders, labels_folders, tokenizer, task)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_folders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating dataframe for folder %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mdataframe_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Concatenate the dataframes to make a total dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/floyd/home/notebooks/preprocess.py\u001b[0m in \u001b[0;36marticles_to_dataframe\u001b[0;34m(article_folder, label_folder, task)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0marticle_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mlabel_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_filenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Initialize sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = generate_training_dataset_from_articles([train_articles], [train_TC_labels], tokenizer, task=\"TC\")\n",
    "global_step, tr_loss = train(train_dataset, model, tokenizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model\n",
    "# model_to_save.save_pretrained(args['output_dir'])\n",
    "# tokenizer.save_pretrained(args['output_dir'])\n",
    "# torch.save(args, os.path.join(args['output_dir'], 'training_args.bin')) # may not need to load\n",
    "\n",
    "\n",
    "model_to_save.save(args['output_dir'])\n",
    "\n",
    "# state = model.state_dict()\n",
    "# torch.save(state, '/floyd/home/datasets/output/TC_model/tc.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_folder = train_articles\n",
    "label_folder = train_TC_labels\n",
    "article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(os.path.join(args['output_dir'], 'pytorch_model.bin'))\n",
    "# model.eval() # vs model.train()\n",
    "# model = \n",
    "# tokenizer = BertTokenizer.from_pretrained(args['output_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_TC_eval_dataset_from_article(article_folder, indices_file, tokenizer):\n",
    "\n",
    "# eval_dataset, eval_dataframe = generate_TC_eval_dataset_from_article(article_filenames[i], tokenizer)\n",
    "\n",
    "# article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_to_dataframe(article_folder, label_folder, task=\"SI\")\n",
    "\n",
    "article_filename = \"/floyd/home/datasets/train-articles/article111111124.txt\"\n",
    "label_filename = \"/floyd/home/datasets/train-labels-task-flc-tc_modified/article111111124.task-flc-tc.labels\"\n",
    "\n",
    "article_id = os.path.basename(article_filename).split(\".\")[0][7:]\n",
    "\n",
    "# Read in the article\n",
    "with codecs.open(article_filename, \"r\", encoding=\"utf8\") as f:\n",
    "    article = f.read()\n",
    "    with open(label_filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        article_sequences = []\n",
    "        labels_list = []\n",
    "        for row in reader:\n",
    "            article_sequences.append(article[int(row[2]):int(row[3])])\n",
    "            labels_list.append(PROP_TECH_TO_LABEL[row[1]])\n",
    "            \n",
    "        \n",
    "    \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_list)\n",
    "PROP_TECH_TO_LABEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.keys() # weights and biases - state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.tensors[0][4] # X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.tensors[1][4] # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%m_%d_%Y-%H.%M.%S\")\n",
    "print(\"date and time =\", dt_string)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SI\n",
    "# articles_folder = dev_articles\n",
    "# # articles_folder = test_articles\n",
    "\n",
    "# article_filenames = sorted(glob.glob(os.path.join(articles_folder, \"*.txt\")))\n",
    "\n",
    "# labels_dir = '/floyd/home/datasets/output/SI_labels'\n",
    "# if not os.path.exists(labels_dir):\n",
    "#     os.makedirs(labels_dir)\n",
    "\n",
    "# for i in range(len(article_filenames)):\n",
    "#     eval_dataset, eval_dataframe = generate_SI_eval_dataset_from_article(article_filenames[i], tokenizer)\n",
    "#     article_id, indices_list = classify_per_article(eval_dataframe, eval_dataset, model, tokenizer)\n",
    "  \n",
    "#     f = open(labels_dir + '/{}-SI.labels'.format(article_id), 'w')\n",
    "#     writer = csv.writer(f, delimiter='\\t')\n",
    "#     for indices in indices_list:\n",
    "#         writer.writerow([article_id, indices[0], indices[1]])\n",
    "#     f.close()\n",
    "\n",
    "# f_total = open('/floyd/home/datasets/output/SI_output/SI_' + dt_string + '.csv', 'w')\n",
    "# writer = csv.writer(f_total, delimiter='\\t')\n",
    "# labels_filenames = sorted(glob.glob(os.path.join(labels_dir, \"*.labels\")))\n",
    "# for i in range(len(labels_filenames)):\n",
    "#     f = open(labels_filenames[i], 'r')\n",
    "#     reader = csv.reader(f, delimiter='\\t')\n",
    "#     for row in reader:\n",
    "#         writer.writerow(row)\n",
    "#     f.close()\n",
    "# f_total.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%m_%d_%y_%H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_folder = test_articles # dev_articles: don't have the template.out file\n",
    "article_filenames = sorted(glob.glob(os.path.join(articles_folder, \"*.txt\")))\n",
    "\n",
    "f = open('/floyd/home/datasets/output/TC_output/output_' + dt_string + '.txt', 'w')\n",
    "\n",
    "\n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "eval_dataset, eval_dataframe = generate_TC_eval_dataset_from_article(test_articles, test_TC_template, tokenizer) #dev = test template\n",
    "predictions = classify_techniques(eval_dataframe, eval_dataset, model, tokenizer)\n",
    "for i in range(len(predictions)):\n",
    "    writer.writerow([eval_dataframe[\"id\"][i], LABEL_TO_PROP_TECH[predictions[i]], eval_dataframe[\"seq_starts\"][i], eval_dataframe[\"seq_ends\"][i]])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def articles_to_dataframe(article_folder, label_folder, task=\"SI\"):\n",
    "\"\"\"\n",
    "Preprocesses the articles into dataframes with sequences with binary tags\n",
    "\"\"\"\n",
    "article_folder = train_articles\n",
    "# First sort the filenames and make sure we have label file for each articles\n",
    "article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels\")))\n",
    "assert len(article_filenames) == len(label_filenames)\n",
    "\n",
    "# Initialize sequences\n",
    "sequences = []\n",
    "\n",
    "# For each article, do:\n",
    "for i in range(len(article_filenames)):\n",
    "    # Get the id name\n",
    "    article_id = os.path.basename(article_filenames[i]).split(\".\")[0][7:]\n",
    "\n",
    "    # Read in the article\n",
    "    with codecs.open(article_filenames[i], \"r\", encoding=\"utf8\") as f:\n",
    "        article = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_to_dataframe(article_folder, label_folder, task=\"SI\")\n",
    "\n",
    "article_filename = \"/floyd/home/datasets/train-articles/article111111117.txt\"\n",
    "label_filename = \"/floyd/home/datasets/train-labels-task-flc-tc/article111111117.task-flc-tc.labels\"\n",
    "\n",
    "article_id = os.path.basename(article_filename).split(\".\")[0][7:]\n",
    "\n",
    "# Read in the article\n",
    "with codecs.open(article_filename, \"r\", encoding=\"utf8\") as f:\n",
    "    article = f.read()\n",
    "    with open(label_filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        article_sequences = []\n",
    "        labels_list = []\n",
    "        for row in reader:\n",
    "            article_sequences.append(article[int(row[2]):int(row[3])])\n",
    "            labels_list.append(PROP_TECH_TO_LABEL[row[1]])\n",
    "            \n",
    "        \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_list)\n",
    "PROP_TECH_TO_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = ['Doubt', 'Repetition', 'Slogans', 'Loaded_Language', \n",
    "        'Exaggeration,Minimisation', 'Name_Calling,Labeling']\n",
    "\n",
    "# Need to adjust config file path for train_TC_labels to re-create the modified folder\n",
    "path = \"/floyd/home/datasets/train-labels-task-flc-tc_modified/\"\n",
    "\n",
    "label_folder = train_TC_labels\n",
    "label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels\")))\n",
    "\n",
    "# For each label, do:\n",
    "for i in range(len(label_filenames)):\n",
    "    # Get the id name\n",
    "    label_id = os.path.basename(label_filenames[i])#.split(\".\")[0][7:]\n",
    "    with open(label_filenames[i],\"r\") as file:\n",
    "        for line in file:\n",
    "            info = line.split(\"\\t\")\n",
    "#             print(info[1])\n",
    "#             print(info)\n",
    "            if any(x in prop for x in info):\n",
    "#                 print('true')\n",
    "                with open(path + label_id,\"a\") as f:\n",
    "                    infoStr = \"\\t\".join(str(x) for x in info)\n",
    "                    f.write(infoStr)\n",
    "            else: \n",
    "                with open(path + label_id,\"w+\") as f:\n",
    "                    pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article_folder = train_articles\n",
    "label_folder = \"/floyd/home/datasets/train-labels-task-flc-tc\"\n",
    "# article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels\")))\n",
    "# 371\n",
    "len(label_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n",
      "File is empty\n"
     ]
    }
   ],
   "source": [
    "path = \"/floyd/home/datasets/train-labels-task-flc-tc/\"\n",
    "\n",
    "# For each label, do:\n",
    "for i in range(len(label_filenames)):\n",
    "    # Get the id name\n",
    "    label_id = os.path.basename(label_filenames[i])#.split(\".\")[0][7:]\n",
    "    if os.stat(path + label_id).st_size == 0:\n",
    "        print('File is empty')\n",
    "        os.remove(path + label_id)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
