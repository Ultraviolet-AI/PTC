{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install --upgrade pip # else errors out\n",
    "# home/tools/data/propaganda-techniques-names-semeval2020task11.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/floyd/home/datasets/train-labels-task-flc-tc'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_TC_labels\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "# Load existing model -- still doesn't work\n",
    "# args['output_dir'] = '/floyd/home/datasets/output/TC_model'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args['output_dir'])\n",
    "# model = AutoModel.from_pretrained(args['output_dir'])\n",
    "# model.to(device)\n",
    "# torch.load(os.path.join(args['output_dir'], 'training_args.bin')) \n",
    "\n",
    "\n",
    "## Bo's method -- better\n",
    "# state = torch.load('/floyd/home/datasets/output/TC_model/tc.pth', map_location=device)\n",
    "\n",
    "# model.load_state_dict(state, strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SI Instantiate the model\n",
    "# config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
    "# config = config_class.from_pretrained(args[\"model_name\"], num_labels=2, finetuning_task=\"binary\")\n",
    "# tokenizer = tokenizer_class.from_pretrained(args[\"model_name\"])\n",
    "# model = model_class(config)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TC Instantiate the model\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
    "config = config_class.from_pretrained(args[\"model_name\"], num_labels=len(PROP_TECH_TO_LABEL))\n",
    "tokenizer = tokenizer_class.from_pretrained(args[\"model_name\"])\n",
    "model = model_class(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args['output_dir'] = '/floyd/home/datasets/output/TC_model'\n",
    "\n",
    "# Make sure to change the output directory to not overwrite SI model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOG:Generating training dataset...\n",
      "INFO:LOG:Generating dataframe for folder /floyd/home/datasets/train-articles\n",
      "INFO:LOG:Creating features from dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0         0  The next transmission could be more pronounced...\n",
      "1         0  when (the plague) comes again it starts from m...\n",
      "2         5                                           appeared\n",
      "3        10                             a very, very different\n",
      "4         1  He also pointed to the presence of the pneumon...\n",
      "...     ...                                                ...\n",
      "6123     11                             the Left killed comedy\n",
      "6124      6  no one looks in the mirror and thinks, ‘this b...\n",
      "6125      9                                Columbia snowflakes\n",
      "6126      6  Comrades, these jokes you have been listening ...\n",
      "6127      6           I'm sure Patel felt very, like, accepted\n",
      "\n",
      "[6128 rows x 2 columns]\n",
      "(6128, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOG:Creating TensorDataset from features dataframe\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb4b371c8b04e1daaa89c9af21b9df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=766.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Epoch: 100%|██████████| 1/1 [04:47<00:00, 287.27s/it]\n",
      "INFO:LOG: global_step = 766, average loss = 1.8908150035002835\n",
      "INFO:LOG:Saving model checkpoint to /floyd/home/datasets/output/TC_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  CHANGE CONFIG FILE PRIOR TO RUNNING\n",
    "train_dataset = generate_training_dataset_from_articles([train_articles], [train_TC_labels], tokenizer, task=\"TC\")\n",
    "global_step, tr_loss = train(train_dataset, model, tokenizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model\n",
    "# model_to_save.save_pretrained(args['output_dir'])\n",
    "# tokenizer.save_pretrained(args['output_dir'])\n",
    "# torch.save(args, os.path.join(args['output_dir'], 'training_args.bin')) # may not need to load\n",
    "\n",
    "\n",
    "# model_to_save.save(args['output_dir'])\n",
    "\n",
    "state = model.state_dict()\n",
    "torch.save(state, '/floyd/home/datasets/output/TC_model/tc.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_TC_eval_dataset_from_article(article_folder, indices_file, tokenizer):\n",
    "\n",
    "# eval_dataset, eval_dataframe = generate_TC_eval_dataset_from_article(article_filenames[i], tokenizer)\n",
    "\n",
    "# article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_to_dataframe(article_folder, label_folder, task=\"SI\")\n",
    "\n",
    "article_filename = \"/floyd/home/datasets/train-articles/article111111124.txt\"\n",
    "label_filename = \"/floyd/home/datasets/train-labels-task-flc-tc_modified/article111111124.task-flc-tc.labels\"\n",
    "\n",
    "article_id = os.path.basename(article_filename).split(\".\")[0][7:]\n",
    "\n",
    "# Read in the article\n",
    "with codecs.open(article_filename, \"r\", encoding=\"utf8\") as f:\n",
    "    article = f.read()\n",
    "    with open(label_filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        article_sequences = []\n",
    "        labels_list = []\n",
    "        for row in reader:\n",
    "            article_sequences.append(article[int(row[2]):int(row[3])])\n",
    "            labels_list.append(PROP_TECH_TO_LABEL[row[1]])\n",
    "            \n",
    "        \n",
    "    \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_list)\n",
    "PROP_TECH_TO_LABEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.keys() # weights and biases - state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.tensors[0][4] # X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.tensors[1][4] # y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SI\n",
    "# articles_folder = dev_articles\n",
    "# # articles_folder = test_articles\n",
    "\n",
    "# article_filenames = sorted(glob.glob(os.path.join(articles_folder, \"*.txt\")))\n",
    "\n",
    "# labels_dir = '/floyd/home/datasets/output/SI_labels'\n",
    "# if not os.path.exists(labels_dir):\n",
    "#     os.makedirs(labels_dir)\n",
    "\n",
    "# for i in range(len(article_filenames)):\n",
    "#     eval_dataset, eval_dataframe = generate_SI_eval_dataset_from_article(article_filenames[i], tokenizer)\n",
    "#     article_id, indices_list = classify_per_article(eval_dataframe, eval_dataset, model, tokenizer)\n",
    "  \n",
    "#     f = open(labels_dir + '/{}-SI.labels'.format(article_id), 'w')\n",
    "#     writer = csv.writer(f, delimiter='\\t')\n",
    "#     for indices in indices_list:\n",
    "#         writer.writerow([article_id, indices[0], indices[1]])\n",
    "#     f.close()\n",
    "\n",
    "# f_total = open('/floyd/home/datasets/output/SI_output/SI_' + dt_string + '.csv', 'w')\n",
    "# writer = csv.writer(f_total, delimiter='\\t')\n",
    "# labels_filenames = sorted(glob.glob(os.path.join(labels_dir, \"*.labels\")))\n",
    "# for i in range(len(labels_filenames)):\n",
    "#     f = open(labels_filenames[i], 'r')\n",
    "#     reader = csv.reader(f, delimiter='\\t')\n",
    "#     for row in reader:\n",
    "#         writer.writerow(row)\n",
    "#     f.close()\n",
    "# f_total.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-01-24 20:08:30.533127\n",
      "date and time = 01_24_21_20:08:30\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%m_%d_%y_%H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOG:Generating evaluation dataset...\n",
      "INFO:LOG:Creating features from dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id seq_starts seq_ends  label  \\\n",
      "0     813452859       1293     1310      0   \n",
      "1     813452859       1801     1843      0   \n",
      "2     813452859       1128     1147      0   \n",
      "3     813452859       1581     1843      0   \n",
      "4     813452859        717      739      0   \n",
      "...         ...        ...      ...    ...   \n",
      "1785  833067493        708      805      0   \n",
      "1786  833067493        200      268      0   \n",
      "1787  833067493        436      468      0   \n",
      "1788  833067493        853      862      0   \n",
      "1789  833067493        940      960      0   \n",
      "\n",
      "                                                   text  \n",
      "0                                     dead in the water  \n",
      "1            this is the world that really exists today  \n",
      "2                                   bitterly against it  \n",
      "3     that's a piece of technology which is manufact...  \n",
      "4                                to dishonour democracy  \n",
      "...                                                 ...  \n",
      "1785  mocked Democrats saying that the 'test results...  \n",
      "1786  Democrats, the test results are back, and Dona...  \n",
      "1787                   along with three laughing emojis  \n",
      "1788                                          trumpeted  \n",
      "1789                               feverish predictions  \n",
      "\n",
      "[1790 rows x 5 columns]\n",
      "(1790, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOG:Creating TensorDataset from features dataframe\n",
      "INFO:LOG:***** Running classification for article 813452859 *****\n",
      "INFO:LOG:  Num sequences = 1790\n",
      "INFO:LOG:  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735ae3123b9042a58a1426315e07180a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=224.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "articles_folder = test_articles # dev_articles: don't have the template.out file\n",
    "article_filenames = sorted(glob.glob(os.path.join(articles_folder, \"*.txt\")))\n",
    "\n",
    "f = open('/floyd/home/datasets/output/TC_output/output_' + dt_string + '.txt', 'w')\n",
    "\n",
    "\n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "eval_dataset, eval_dataframe = generate_TC_eval_dataset_from_article(test_articles, test_TC_template, tokenizer) #dev = test template\n",
    "predictions = classify_techniques(eval_dataframe, eval_dataset, model, tokenizer)\n",
    "for i in range(len(predictions)):\n",
    "    writer.writerow([eval_dataframe[\"id\"][i], LABEL_TO_PROP_TECH[predictions[i]], eval_dataframe[\"seq_starts\"][i], eval_dataframe[\"seq_ends\"][i]])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create modified labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = ['Loaded_Language'] #, 'Doubt', 'Repetition', 'Slogans', \n",
    "#         'Exaggeration,Minimisation', 'Name_Calling,Labeling']\n",
    "\n",
    "# Need to adjust config file path for train_TC_labels to re-create the modified folder\n",
    "path = \"/floyd/home/datasets/train-labels-task-flc-tc_modified/\"\n",
    "\n",
    "label_folder = train_TC_labels\n",
    "label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels\")))\n",
    "\n",
    "# For each label, do:\n",
    "for i in range(len(label_filenames)):\n",
    "    # Get the id name\n",
    "    label_id = os.path.basename(label_filenames[i])#.split(\".\")[0][7:]\n",
    "    with open(label_filenames[i],\"r\") as file:\n",
    "        for line in file:\n",
    "            info = line.split(\"\\t\")\n",
    "#             print(info[1])\n",
    "#             print(info)\n",
    "            if any(x in prop for x in info):\n",
    "#                 print('true')\n",
    "                with open(path + label_id,\"a\") as f:\n",
    "                    infoStr = \"\\t\".join(str(x) for x in info)\n",
    "                    f.write(infoStr)\n",
    "            else: \n",
    "                with open(path + label_id,\"w+\") as f:\n",
    "                    pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty files\n",
    "\n",
    "Dev labels are half of the dev articles -- waste of time  \n",
    "DO NOT NEED TO RE-RUN. THIS IS A ONE TIME MODIFICATION TO DROP ORIGINAL DATA THAT HAD ZERO LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "article_folder = train_articles #dev_articles \n",
    "label_folder = \"/floyd/home/datasets/train-labels-task-flc-tc_modified\"\n",
    "# label_folder = \"/floyd/home/datasets/train-labels-task-flc-tc\"\n",
    "# label_folder = '/floyd/home/datasets/dev-labels-task-flc-tc/'\n",
    "article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels\")))\n",
    "\n",
    "print(len(article_filenames))\n",
    "print(len(label_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/floyd/home/datasets/train-labels-task-flc-tc/\"\n",
    "# path = '/floyd/home/datasets/dev-labels-task-flc-tc/'\n",
    "\n",
    "article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "# For each label, do:\n",
    "for i in range(len(label_filenames)):\n",
    "    # Get the id name\n",
    "    label_id = os.path.basename(label_filenames[i])#.split(\".\")[0][7:]\n",
    "    article_id = os.path.basename(article_filenames[i])#.split(\".\")[0][7:]\n",
    "    if os.stat(path + label_id).st_size == 0:\n",
    "        print('File is empty -> ' + label_id)\n",
    "        print(article_id)\n",
    "        os.remove(path + label_id)\n",
    "        os.remove('/floyd/home/datasets/train-articles/' + article_id)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_folder = train_articles\n",
    "label_folder = \"/floyd/home/datasets/train-labels-task-flc-tc_modified\"\n",
    "# label_folder = \"/floyd/home/datasets/train-labels-task-flc-tc\"\n",
    "article_filenames = sorted(glob.glob(os.path.join(article_folder, \"*.txt\")))\n",
    "label_filenames = sorted(glob.glob(os.path.join(label_folder, \"*.labels\")))\n",
    "# 371\n",
    "len(label_filenames) == len(article_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
